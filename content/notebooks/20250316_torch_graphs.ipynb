{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a396180d-5499-4e68-b82f-1d8f544b1b76",
   "metadata": {},
   "source": [
    "# PyTorch Graphs Three Ways: Data-Dependent Control Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bde17a-e66a-4914-9d63-cc6943825d5e",
   "metadata": {},
   "source": [
    "Over the past few years, PyTorch went through a few iterations for turning Python code into a graph to improve performance:\n",
    "\n",
    "1. **TorchScript** can trace or parse your code to generate a TorchScript intermediate representation that works on a subset of Python. **Not in active development**.\n",
    "2. **FX Graphs**: `torch.fx.symbolic_trace` traces your code to produce a FX Graph we can mutate for optimizations.\n",
    "3. **Torch Compile**: `torch.compile` reads the Python bytecode to generate FX Graphs while also falling back to Python for code it does not recognize.\n",
    "\n",
    "This blog looks at how each system handles data dependent control flow with a simple example. You can run the code in this [post on Google Colab](https://colab.research.google.com/github/thomasjpfan/thomasjpfan.github.io/blob/main/content/notebooks/20250316_torch_graphs.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f82af-42af-421d-b773-455198162829",
   "metadata": {},
   "source": [
    "## Simple Function\n",
    "\n",
    "First, we define a simple function that branches based on the input's data, i.e. if the input contains all positive values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51dcc76-322f-4992-b34d-5398aeb4cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def func(x: torch.Tensor):\n",
    "    all_pos = torch.all(x >= 0)\n",
    "    if all_pos:\n",
    "        return x + 10\n",
    "    else:\n",
    "        return x - 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb84af-5fe9-49ce-b486-338386b5afe8",
   "metadata": {},
   "source": [
    "here, we pass all positive and all negative tensors to show the different code paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42a7d4a-4033-49da-879f-cfcefc21180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos = torch.asarray([1, 2, 3])\n",
    "x_neg = torch.asarray([-1, -2, -3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454d9f25-8b1c-49b8-be71-17a26dd28100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-11, -12, -13]), tensor([11, 12, 13]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(x_neg), func(x_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb69b0-f3cb-4ac0-8f76-e26cf89b10d2",
   "metadata": {},
   "source": [
    "## TorchScript\n",
    "\n",
    "TorchScript allows us to trace a function, by passing in a sample input and running `jit.trace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f890e9d-45a5-413b-9d77-da20df9f6bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/pvs3_wlj23z_qxd4m8dv9w300000gn/T/ipykernel_31962/766430457.py:5: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if all_pos:\n"
     ]
    }
   ],
   "source": [
    "func_jit_trace = torch.jit.trace(func, (x_pos,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324066d-553b-4dbc-ad03-a2c01a7feac7",
   "metadata": {},
   "source": [
    "We see a warning because it notices the control flow and that only one of the branches will be traced. This means that the resulting function is incorrect for a negative tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01583b41-5348-49df-802e-c6cb40a2063e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([11, 12, 13]), tensor([9, 8, 7]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_jit_trace(x_pos), func_jit_trace(x_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf17ed-e289-4dee-bf84-ecf64bb5cea1",
   "metadata": {},
   "source": [
    "Note, TorchScript also has a `jit.script` which can parse the control flow logic and give the correct result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "588b0152-c72b-4e3d-9ced-eadacd1a55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_jit_script = torch.jit.script(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6c0d2c-0ff7-4cb9-9a83-6d62947b230e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-11, -12, -13]), tensor([11, 12, 13]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_jit_script(x_neg), func_jit_script(x_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec039b27-825e-43af-9d18-f49eee5050f5",
   "metadata": {},
   "source": [
    "## Torch FX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcbfa8-e6c7-44db-b98b-84073f18a45d",
   "metadata": {},
   "source": [
    "When we run `torch.fx.symbolic_trace` on the same function, it'll throw an error because it can not convert the function into a single FX graph because of the control flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cce0e53-7f06-4260-9781-7a83d8c8aa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbolically traced variables cannot be used as inputs to control flow\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    torch.fx.symbolic_trace(func)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ceaa8b-608c-4d38-b611-05272bea906f",
   "metadata": {},
   "source": [
    "A workaround is to provide a `concrete_args`, so the tracing only goes down one of the code paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb25b72-0cb3-463b-ad91-a7ab0de58670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasfan/micromamba/envs/torch/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py:906: UserWarning: Was not able to add assertion to guarantee correct input x to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fx_func = torch.fx.symbolic_trace(func, concrete_args={\"x\": x_pos})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a645072d-1cd5-452b-8da4-399d3c0db48b",
   "metadata": {},
   "source": [
    "But running this functino will give the incorrect results for `x_neg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "822a41e6-db21-435a-837f-967f2c724ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([11, 12, 13]), tensor([11, 12, 13]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_func(x_neg), fx_func(x_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945b8bc-5409-484d-9a85-c61d9e51ccfe",
   "metadata": {},
   "source": [
    "## torch.compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf624b-20e3-4a5a-90e4-7c7726e65496",
   "metadata": {},
   "source": [
    "The newer `torch.compile` will compile the function and gives the correct results by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eef550c9-44a9-447a-b6bf-74bc5143e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_func = torch.compile(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "688bad5a-7d47-4a32-8143-bfc5f3778c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-11, -12, -13]), tensor([11, 12, 13]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_func(x_neg), compiled_func(x_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d37e17-1e41-4eea-bd8b-3b761e976aba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Under the covers, `torch.compile` builds two graphs and uses a graph break to handle the control flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7537ab3-6310-4c72-89dc-b8d8e237fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs: 2\n",
      "Graph Breaks: 1\n"
     ]
    }
   ],
   "source": [
    "explained = torch._dynamo.explain(func)(x_pos)\n",
    "\n",
    "print(f\"\"\"Graphs: {explained.graph_count}\n",
    "Graph Breaks: {explained.graph_break_count}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646fa71-94f2-40f2-9dc0-acfd0c61703b",
   "metadata": {},
   "source": [
    "If you want the most performance, then it's best to avoid the graph breaks, using `fullgraph=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a0f5ddc-d5d3-4a9e-b6c8-90e21e5403cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_full_bad = torch.compile(func, fullgraph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f89039-a6e5-499d-9453-2b8c28555f93",
   "metadata": {},
   "source": [
    "But this will result in an error because the conditional requires a graph break:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb4a9d99-68ea-4e17-beb6-73dc60aace18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic control flow is not supported at the moment. Please use functorch.experimental.control_flow.cond to explicitly capture the control flow. For more information about this error, see: https://pytorch.org/docs/main/generated/exportdb/index.html#cond-operands\n",
      "\n",
      "from user code:\n",
      "   File \"/var/folders/9l/pvs3_wlj23z_qxd4m8dv9w300000gn/T/ipykernel_31962/766430457.py\", line 5, in func\n",
      "    if all_pos:\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    compiled_full_bad(x_pos)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21fac6b-eb4b-4cad-8303-d336db5312f1",
   "metadata": {},
   "source": [
    "To work around the graph break, we can use `torch.cond` to build the full graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de9706e6-59eb-405d-b6e6-62082a547bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_cond(x):\n",
    "    return torch.cond(\n",
    "        torch.all(x >= 0), lambda x: x + 10, lambda x: x - 10, (x,)\n",
    "    )\n",
    "\n",
    "compiled_full_good = torch.compile(func_cond, fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9087f0b0-e2c3-44f0-84dd-16ff48212fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-11, -12, -13]), tensor([11, 12, 13]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_full_good(x_neg), compiled_full_good(x_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4002b7-894c-458d-96e0-4859f307e19f",
   "metadata": {},
   "source": [
    "We see that there are zero graph breaks by using `explain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb0338e1-dbde-4ea3-bc83-dc5b961884a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs: 1\n",
      "Graph Breaks: 0\n"
     ]
    }
   ],
   "source": [
    "explained_cond = torch._dynamo.explain(func_cond)(x_pos)\n",
    "\n",
    "print(f\"\"\"Graphs: {explained_cond.graph_count}\n",
    "Graph Breaks: {explained_cond.graph_break_count}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5daee1d-c291-414e-b4e4-ba780b412ea8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "TorchScript’s usability issue stems from only supporting a subset of Python, which means we likely need to update code to make it work. FX Graph has a limited symbolic tracer producing an intermediate representation that we can mutate and passed to a compiler.\n",
    "\n",
    "Using Python’s bytecode, `torch.compile` still produces FX graphs, but it “just work” with any other Python code. For code `torch.compile` does not recognize it will fall back to the Python interpreter. This means you can `torch.compile` to get some initial improvements and iterate to make your code run faster by reducing the number of graph breaks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
